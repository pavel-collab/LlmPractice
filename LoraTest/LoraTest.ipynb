{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KaMcFqLBh1-R"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MsLIUumRxhfz"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jXUnv3k7wu9C"
   },
   "outputs": [],
   "source": [
    "model_name = \"roberta-large\"\n",
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8HBqOm_xbx5"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"mehdiiraqui/twitter_disaster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tkEqJCRoxma5"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation datasets\n",
    "data = dataset['train'].train_test_split(train_size=0.8, seed=42)\n",
    "# Rename the default \"test\" split to \"validation\"\n",
    "data['val'] = data.pop(\"test\")\n",
    "# Convert the test dataframe to HuggingFace dataset and add it into the first dataset\n",
    "data['test'] = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wh87j6ZZxoRR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GnOcigZxrel"
   },
   "outputs": [],
   "source": [
    "data['train'].to_pandas().info()\n",
    "data['test'].to_pandas().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hjm5lyECxuo-"
   },
   "outputs": [],
   "source": [
    "pos_weights = len(data['train'].to_pandas()) / (2 * data['train'].to_pandas().target.value_counts()[1])\n",
    "neg_weights = len(data['train'].to_pandas()) / (2 * data['train'].to_pandas().target.value_counts()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FUrCdXFGx0bl"
   },
   "outputs": [],
   "source": [
    "POS_WEIGHT, NEG_WEIGHT = (1.1637114032405993, 0.8766697374481806)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7IvE9TJx4eY"
   },
   "outputs": [],
   "source": [
    "# Number of Characters\n",
    "max_char = data['train'].to_pandas()['text'].str.len().max()\n",
    "# Number of Words\n",
    "max_words = data['train'].to_pandas()['text'].str.split().str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nrUDjFrdx6yc"
   },
   "outputs": [],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eUY4WLLmx7IO"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6U4DDpY-x9Y2"
   },
   "outputs": [],
   "source": [
    "roberta_tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvoTOt_Fx_IO"
   },
   "outputs": [],
   "source": [
    "def roberta_preprocessing_function(examples):\n",
    "    return roberta_tokenizer(examples['text'], truncation=True, max_length=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WRRbiBOFyFqa"
   },
   "outputs": [],
   "source": [
    "roberta_preprocessing_function(data['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRpYLI-dyIu0"
   },
   "outputs": [],
   "source": [
    "col_to_delete = ['id', 'keyword','location', 'text']\n",
    "# Apply the preprocessing function and remove the undesired columns\n",
    "roberta_tokenized_datasets = data.map(roberta_preprocessing_function, batched=True, remove_columns=col_to_delete)\n",
    "# Rename the target to label as for HugginFace standards\n",
    "roberta_tokenized_datasets = roberta_tokenized_datasets.rename_column(\"target\", \"label\")\n",
    "# Set to torch format\n",
    "roberta_tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPGZfggKyNUk"
   },
   "outputs": [],
   "source": [
    "roberta_tokenized_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IHt3eSwkyNyW"
   },
   "outputs": [],
   "source": [
    "# Data collator for padding a batch of examples to the maximum length seen in the batch\n",
    "from transformers import DataCollatorWithPadding\n",
    "roberta_data_collator = DataCollatorWithPadding(tokenizer=roberta_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fGeJTU9yQ0S"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "roberta_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9MftzihoyUm5"
   },
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "roberta_peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, r=2, lora_alpha=16, lora_dropout=0.1, bias=\"none\",\n",
    ")\n",
    "roberta_model = get_peft_model(roberta_model, roberta_peft_config)\n",
    "roberta_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2YOGFcw0zFr"
   },
   "outputs": [],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DenAgveNyXuO"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # All metrics are already predefined in the HF `evaluate` package\n",
    "    precision_metric = evaluate.load(\"precision\")\n",
    "    recall_metric = evaluate.load(\"recall\")\n",
    "    f1_metric= evaluate.load(\"f1\")\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    logits, labels = eval_pred # eval_pred is the tuple of predictions and labels returned by the model\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels)[\"precision\"]\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels)[\"recall\"]\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    # The trainer is expecting a dictionary where the keys are the metrics names and the values are the scores.\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, 'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42-FZ4LSyeaV"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "class WeightedCELossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # Get model's predictions\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # Compute custom loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=torch.tensor([neg_weights, pos_weights], device=model.device, dtype=logits.dtype))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eHg68CEJyiWj"
   },
   "outputs": [],
   "source": [
    "roberta_model = roberta_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "edZy-sLPykxX"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "lr = 1e-4\n",
    "batch_size = 8\n",
    "num_epochs = 5\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"roberta-large-lora-token-classification\",\n",
    "    learning_rate=lr,\n",
    "    lr_scheduler_type= \"constant\",\n",
    "    warmup_ratio= 0.1,\n",
    "    max_grad_norm= 0.3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.001,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"wandb\",\n",
    "    fp16=False,\n",
    "    gradient_checkpointing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_kxqVZ3ypqV"
   },
   "outputs": [],
   "source": [
    "roberta_trainer = WeightedCELossTrainer(\n",
    "    model=roberta_model,\n",
    "    args=training_args,\n",
    "    train_dataset=roberta_tokenized_datasets['train'],\n",
    "    eval_dataset=roberta_tokenized_datasets[\"val\"],\n",
    "    data_collator=roberta_data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_fQGKLvy7Zg"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mN2v5nZSyvlM"
   },
   "outputs": [],
   "source": [
    "date = datetime.strftime(datetime.now(), \"%d.%m.%Y-%H.%M.%S\")\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"using-lora\",\n",
    "    name=f\"{model_name}-cls-{date}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUFP63ApzCj2"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    roberta_trainer.train()\n",
    "except RuntimeError:\n",
    "    ...\n",
    "\n",
    "# Закрыть сессию WandB\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
